import torch
import torchvision
import torchvision.datasets as dset
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from coco_caption.pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer
from nltk import tokenize
#from pycocotools.coco import COCO
#from collections import Counter
#
#coco = COCO('../data/train/captions_train2014.json')
#print('num of original train images: ' + str(len(coco.imgs)))
#anns_keys = coco.anns.keys()
#
#original_token = []
#
#for key in anns_keys:
#	caption = coco.anns[key]['caption']
#	tokens = nltk.tokenize.word_tokenize(caption.lower())
#	if tokens[-1] == '.':
#		tokens = tokens[:-1]
#	if len(tokens) <= 13:
#		img_id = coco.anns[key]['image_id']
#		tmp = [coco.loadImgs(img_id)[0]['file_name'], tokens]
#		original_token.append(tmp)
#
#freq = Counter()
#for i in range(len(original_token)):
#	freq.update(set(original_token[i][1]))
#
#common = freq.most_common()
#vocab = sorted([t for t,c in common in c>=3])
#vocab.append('<pad>')
#vocab.append('<start>')
#vocab.append('<end>')
#vocab.append('<unk>')
#
#id_to_word = {i+1:t for i,t in enumerate(vocab)}
#word_to_id = {t:i+1 for i,t in enumerate(vocab)}
#
#original_token_id = {}
#
#for i in range(len(original_token)):
#	sent = ['<start>'] + originl_token[i][1] + ['<end>']
#	sent_id = [word_to_id[t] if (t in word_to_id) else word_to_id['<unk>'] for t in sent]
#	if (len(sent_id)) < 15:
#		sent_id = sent_id + [word_to_id['<pad>']] * (15-len(sent_id))
#	original_token_id.append([original_token[i][0], sent_id])
#
#with open('id_to_word.pkl', 'wb') as f:
#	pickle.dump(id_to_word, f)
#
#with open('word_to_id.pkl', 'wb') as f:
#	pickle.dump(word_to_id, f)
#
#with open('filename_token.pkl', 'wb') as f:
#	pickle.dump(original_token_id, f)
#
#print('num of train images: ' + str(len(list(set([c[0] for c in filename_token])))))
#print('num of vocabs: ' + str(len(word_to_id)))

def save_img(img_name, img):
	img_np = img.to('cpu').detach().numpy().copy()
	plt.imsave(img_name, img_np[0])

def preprocess_caption(data):
	for i in range(len(data)):
		print("a")	

img_size = (299,299)

trans = transforms.Compose([transforms.Resize(img_size),transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])
trainset = dset.CocoCaptions(root = './data/train/images',
                             annFile = './data/train/captions_train2014.json',
                             transform=trans)
trainloader = torch.utils.data.DataLoader(trainset, batch_size = 2048, shuffle = True, num_workers = 4)

print('Number of samples: ', len(trainset))
img, target = trainset[3]
#save_img("bbb.jpg", img)
print('Image size: ', img.size())
print(target)
print(trainloader)
print(trainset[3][1])

valset = dset.CocoCaptions(root = './data/val/images',
                           annFile = './data/val/captions_val2014.json',
                           transform=trans)
valloader = torch.utils.data.DataLoader(valset, batch_size = 2048, shuffle = False, num_workers = 4)

print('Number of samples: ', len(valset))
img, target = valset[3]
print('Image size: ', img.size())
print(target)
print(valloader)

from vocab import vocab
vocab.preprocess_caption()
